{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"D:\\corona\\heart_Disease\\heart.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      "age         303 non-null int64\n",
      "sex         303 non-null int64\n",
      "cp          303 non-null int64\n",
      "trestbps    303 non-null int64\n",
      "chol        303 non-null int64\n",
      "fbs         303 non-null int64\n",
      "restecg     303 non-null int64\n",
      "thalach     303 non-null int64\n",
      "exang       303 non-null int64\n",
      "oldpeak     303 non-null float64\n",
      "slope       303 non-null int64\n",
      "ca          303 non-null int64\n",
      "thal        303 non-null int64\n",
      "target      303 non-null int64\n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(data, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('target', axis=1).values\n",
    "y = data['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=40, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_r = RandomForestClassifier(n_estimators=1000, n_jobs = -1, random_state = 40)\n",
    "model_r.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_r.predict(X_train)\n",
    "y_test_pred = model_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  6],\n",
       "       [ 0, 29]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the accuracy of random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9016393442622951"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, model_r.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/200\n",
      "242/242 [==============================] - 1s 5ms/sample - loss: 0.6831 - val_loss: 0.6717\n",
      "Epoch 2/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6467 - val_loss: 0.6060\n",
      "Epoch 3/200\n",
      "242/242 [==============================] - 0s 173us/sample - loss: 0.5953 - val_loss: 0.5440\n",
      "Epoch 4/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.5362 - val_loss: 0.4795\n",
      "Epoch 5/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4757 - val_loss: 0.4283\n",
      "Epoch 6/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.4274 - val_loss: 0.3906\n",
      "Epoch 7/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4043 - val_loss: 0.3742\n",
      "Epoch 8/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.3869 - val_loss: 0.3571\n",
      "Epoch 9/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.3864 - val_loss: 0.3447\n",
      "Epoch 10/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.3686 - val_loss: 0.3416\n",
      "Epoch 11/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.3554 - val_loss: 0.3629\n",
      "Epoch 12/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.3573 - val_loss: 0.3370\n",
      "Epoch 13/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.3480 - val_loss: 0.3233\n",
      "Epoch 14/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.3430 - val_loss: 0.3288\n",
      "Epoch 15/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.3396 - val_loss: 0.3383\n",
      "Epoch 16/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.3286 - val_loss: 0.3196\n",
      "Epoch 17/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.3184 - val_loss: 0.3404\n",
      "Epoch 18/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.3287 - val_loss: 0.3109\n",
      "Epoch 19/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.3288 - val_loss: 0.3315\n",
      "Epoch 20/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.3128 - val_loss: 0.3245\n",
      "Epoch 21/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.3007 - val_loss: 0.3152\n",
      "Epoch 22/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2939 - val_loss: 0.3390\n",
      "Epoch 23/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.2994 - val_loss: 0.3220\n",
      "Epoch 24/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.2853 - val_loss: 0.3257\n",
      "Epoch 25/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.2778 - val_loss: 0.3341\n",
      "Epoch 26/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.2699 - val_loss: 0.3278\n",
      "Epoch 27/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2661 - val_loss: 0.3230\n",
      "Epoch 28/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.2604 - val_loss: 0.3352\n",
      "Epoch 29/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2599 - val_loss: 0.3201\n",
      "Epoch 30/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2571 - val_loss: 0.3203\n",
      "Epoch 31/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2425 - val_loss: 0.3354\n",
      "Epoch 32/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2439 - val_loss: 0.3289\n",
      "Epoch 33/200\n",
      "242/242 [==============================] - 0s 190us/sample - loss: 0.2352 - val_loss: 0.3789\n",
      "Epoch 34/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.2356 - val_loss: 0.3344\n",
      "Epoch 35/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.2186 - val_loss: 0.3518\n",
      "Epoch 36/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.2183 - val_loss: 0.3309\n",
      "Epoch 37/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.2247 - val_loss: 0.4099\n",
      "Epoch 38/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.2138 - val_loss: 0.3418\n",
      "Epoch 39/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.2485 - val_loss: 0.4281\n",
      "Epoch 40/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.2214 - val_loss: 0.3289\n",
      "Epoch 41/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.1944 - val_loss: 0.3915\n",
      "Epoch 42/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.1913 - val_loss: 0.3380\n",
      "Epoch 43/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.1874 - val_loss: 0.4264\n",
      "Epoch 44/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.1898 - val_loss: 0.3436\n",
      "Epoch 45/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.1827 - val_loss: 0.3809\n",
      "Epoch 46/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.1698 - val_loss: 0.3425\n",
      "Epoch 47/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.1666 - val_loss: 0.3834\n",
      "Epoch 48/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.1628 - val_loss: 0.3579\n",
      "Epoch 49/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.1771 - val_loss: 0.3555\n",
      "Epoch 50/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.1633 - val_loss: 0.4372\n",
      "Epoch 51/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.1602 - val_loss: 0.3756\n",
      "Epoch 52/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.1528 - val_loss: 0.4454\n",
      "Epoch 53/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.1474 - val_loss: 0.3857\n",
      "Epoch 54/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.1374 - val_loss: 0.4040\n",
      "Epoch 55/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.1294 - val_loss: 0.3955\n",
      "Epoch 56/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 0.1282 - val_loss: 0.4472\n",
      "Epoch 57/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.1283 - val_loss: 0.4011\n",
      "Epoch 58/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.1309 - val_loss: 0.4060\n",
      "Epoch 59/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.1233 - val_loss: 0.4257\n",
      "Epoch 60/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.1220 - val_loss: 0.4818\n",
      "Epoch 61/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.1263 - val_loss: 0.4149\n",
      "Epoch 62/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.1164 - val_loss: 0.5455\n",
      "Epoch 63/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.1226 - val_loss: 0.4492\n",
      "Epoch 64/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.1123 - val_loss: 0.4153\n",
      "Epoch 65/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.1111 - val_loss: 0.5205\n",
      "Epoch 66/200\n",
      "242/242 [==============================] - 0s 119us/sample - loss: 0.1076 - val_loss: 0.4829\n",
      "Epoch 67/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0960 - val_loss: 0.4872\n",
      "Epoch 68/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.0907 - val_loss: 0.4766\n",
      "Epoch 69/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0873 - val_loss: 0.5195\n",
      "Epoch 70/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0845 - val_loss: 0.4911\n",
      "Epoch 71/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0872 - val_loss: 0.6401\n",
      "Epoch 72/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.1006 - val_loss: 0.5020\n",
      "Epoch 73/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0757 - val_loss: 0.5179\n",
      "Epoch 74/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0755 - val_loss: 0.5688\n",
      "Epoch 75/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0742 - val_loss: 0.5580\n",
      "Epoch 76/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0695 - val_loss: 0.5370\n",
      "Epoch 77/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0660 - val_loss: 0.6329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0778 - val_loss: 0.5382\n",
      "Epoch 79/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0580 - val_loss: 0.6398\n",
      "Epoch 80/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0695 - val_loss: 0.5924\n",
      "Epoch 81/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0633 - val_loss: 0.5681\n",
      "Epoch 82/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0644 - val_loss: 0.6873\n",
      "Epoch 83/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0637 - val_loss: 0.6007\n",
      "Epoch 84/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0543 - val_loss: 0.6069\n",
      "Epoch 85/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0590 - val_loss: 0.7499\n",
      "Epoch 86/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0970 - val_loss: 0.5778\n",
      "Epoch 87/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0865 - val_loss: 0.6050\n",
      "Epoch 88/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0565 - val_loss: 0.6413\n",
      "Epoch 89/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0500 - val_loss: 0.6244\n",
      "Epoch 90/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0454 - val_loss: 0.7109\n",
      "Epoch 91/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0471 - val_loss: 0.6549\n",
      "Epoch 92/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0575 - val_loss: 0.6198\n",
      "Epoch 93/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.0571 - val_loss: 0.8322\n",
      "Epoch 94/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0594 - val_loss: 0.6407\n",
      "Epoch 95/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0373 - val_loss: 0.7115\n",
      "Epoch 96/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0507 - val_loss: 0.7299\n",
      "Epoch 97/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0637 - val_loss: 0.6227\n",
      "Epoch 98/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 0.0492 - val_loss: 0.7555\n",
      "Epoch 99/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0466 - val_loss: 0.6589\n",
      "Epoch 100/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0403 - val_loss: 0.7184\n",
      "Epoch 101/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.0326 - val_loss: 0.7055\n",
      "Epoch 102/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0250 - val_loss: 0.7400\n",
      "Epoch 103/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0301 - val_loss: 0.7459\n",
      "Epoch 104/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0274 - val_loss: 0.7472\n",
      "Epoch 105/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0231 - val_loss: 0.7367\n",
      "Epoch 106/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0249 - val_loss: 0.7590\n",
      "Epoch 107/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0220 - val_loss: 0.7821\n",
      "Epoch 108/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0192 - val_loss: 0.7880\n",
      "Epoch 109/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0192 - val_loss: 0.7867\n",
      "Epoch 110/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0178 - val_loss: 0.8129\n",
      "Epoch 111/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0240 - val_loss: 0.7562\n",
      "Epoch 112/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0258 - val_loss: 0.8487\n",
      "Epoch 113/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0173 - val_loss: 0.7906\n",
      "Epoch 114/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0214 - val_loss: 0.8375\n",
      "Epoch 115/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0242 - val_loss: 0.8299\n",
      "Epoch 116/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0291 - val_loss: 0.8170\n",
      "Epoch 117/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0321 - val_loss: 0.9111\n",
      "Epoch 118/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0211 - val_loss: 0.8298\n",
      "Epoch 119/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0237 - val_loss: 0.9500\n",
      "Epoch 120/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0185 - val_loss: 0.8594\n",
      "Epoch 121/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0208 - val_loss: 0.8424\n",
      "Epoch 122/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0140 - val_loss: 0.9019\n",
      "Epoch 123/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0150 - val_loss: 0.9019\n",
      "Epoch 124/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0106 - val_loss: 0.9192\n",
      "Epoch 125/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.0119 - val_loss: 0.9181\n",
      "Epoch 126/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.0116 - val_loss: 0.9288\n",
      "Epoch 127/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0097 - val_loss: 0.9363\n",
      "Epoch 128/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0091 - val_loss: 0.9399\n",
      "Epoch 129/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0081 - val_loss: 0.9497\n",
      "Epoch 130/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0070 - val_loss: 0.9919\n",
      "Epoch 131/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0077 - val_loss: 0.9612\n",
      "Epoch 132/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0065 - val_loss: 0.9941\n",
      "Epoch 133/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0072 - val_loss: 0.9816\n",
      "Epoch 134/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0059 - val_loss: 1.0127\n",
      "Epoch 135/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0088 - val_loss: 0.9789\n",
      "Epoch 136/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0107 - val_loss: 1.0397\n",
      "Epoch 137/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0147 - val_loss: 0.9859\n",
      "Epoch 138/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.0130 - val_loss: 1.0526\n",
      "Epoch 139/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0115 - val_loss: 1.0113\n",
      "Epoch 140/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0095 - val_loss: 1.0815\n",
      "Epoch 141/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0136 - val_loss: 1.0011\n",
      "Epoch 142/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0085 - val_loss: 1.1500\n",
      "Epoch 143/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0161 - val_loss: 0.9476\n",
      "Epoch 144/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0300 - val_loss: 1.1972\n",
      "Epoch 145/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0181 - val_loss: 1.0390\n",
      "Epoch 146/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0170 - val_loss: 1.0271\n",
      "Epoch 147/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.0288 - val_loss: 1.1255\n",
      "Epoch 148/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0235 - val_loss: 1.0136\n",
      "Epoch 149/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0082 - val_loss: 1.1092\n",
      "Epoch 150/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 0.0110 - val_loss: 1.0463\n",
      "Epoch 151/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0080 - val_loss: 1.1258\n",
      "Epoch 152/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0085 - val_loss: 1.0268\n",
      "Epoch 153/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0053 - val_loss: 1.1018\n",
      "Epoch 154/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0066 - val_loss: 1.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0051 - val_loss: 1.0586\n",
      "Epoch 156/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0044 - val_loss: 1.0885\n",
      "Epoch 157/200\n",
      "242/242 [==============================] - 0s 173us/sample - loss: 0.0032 - val_loss: 1.0521\n",
      "Epoch 158/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0032 - val_loss: 1.1012\n",
      "Epoch 159/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0028 - val_loss: 1.1006\n",
      "Epoch 160/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0025 - val_loss: 1.1086\n",
      "Epoch 161/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0023 - val_loss: 1.1214\n",
      "Epoch 162/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0020 - val_loss: 1.1209\n",
      "Epoch 163/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0020 - val_loss: 1.1325\n",
      "Epoch 164/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0021 - val_loss: 1.1381\n",
      "Epoch 165/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0019 - val_loss: 1.1493\n",
      "Epoch 166/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0018 - val_loss: 1.1444\n",
      "Epoch 167/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0018 - val_loss: 1.1514\n",
      "Epoch 168/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 0.0018 - val_loss: 1.1685\n",
      "Epoch 169/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0017 - val_loss: 1.1666\n",
      "Epoch 170/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0018 - val_loss: 1.1666\n",
      "Epoch 171/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0016 - val_loss: 1.1823\n",
      "Epoch 172/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0016 - val_loss: 1.1783\n",
      "Epoch 173/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0016 - val_loss: 1.1821\n",
      "Epoch 174/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0015 - val_loss: 1.1901\n",
      "Epoch 175/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0015 - val_loss: 1.1885\n",
      "Epoch 176/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 0.0015 - val_loss: 1.1905\n",
      "Epoch 177/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0014 - val_loss: 1.2035\n",
      "Epoch 178/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0014 - val_loss: 1.2040\n",
      "Epoch 179/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 1.2089\n",
      "Epoch 180/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0013 - val_loss: 1.2101\n",
      "Epoch 181/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0014 - val_loss: 1.2183\n",
      "Epoch 182/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0013 - val_loss: 1.2251\n",
      "Epoch 183/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0013 - val_loss: 1.2269\n",
      "Epoch 184/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0012 - val_loss: 1.2174\n",
      "Epoch 185/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0013 - val_loss: 1.2281\n",
      "Epoch 186/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0012 - val_loss: 1.2298\n",
      "Epoch 187/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0011 - val_loss: 1.2320\n",
      "Epoch 188/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0011 - val_loss: 1.2518\n",
      "Epoch 189/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.0011 - val_loss: 1.2523\n",
      "Epoch 190/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.0011 - val_loss: 1.2377\n",
      "Epoch 191/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.0011 - val_loss: 1.2486\n",
      "Epoch 192/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.0011 - val_loss: 1.2581\n",
      "Epoch 193/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.0010 - val_loss: 1.2616\n",
      "Epoch 194/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.0010 - val_loss: 1.2608\n",
      "Epoch 195/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.0010 - val_loss: 1.2706\n",
      "Epoch 196/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 9.6262e-04 - val_loss: 1.2713\n",
      "Epoch 197/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 9.9710e-04 - val_loss: 1.2714\n",
      "Epoch 198/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 9.1155e-04 - val_loss: 1.2838\n",
      "Epoch 199/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 9.7828e-04 - val_loss: 1.2908\n",
      "Epoch 200/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 9.1963e-04 - val_loss: 1.2772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c8ab137288>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100,activation='relu'))         # 100 : Number of neurons     We can also write it as : units = 100\n",
    "model.add(Dense(50,activation='relu'))          # 50 : Number of neurons   \n",
    "model.add(Dense(25,activation='relu'))          # 25 : Number of neurons\n",
    "model.add(Dense(15,activation='relu'))          # 15 : Number of neurons\n",
    "model.add(Dense(5,activation='relu'))           # 5 : Number of neurons\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))        # activation=\"sigmoid\" : bcz it represents sigmoid curve i.e 1 or 0\n",
    "\n",
    "# After each epochs of training our X_train we will run test data check our losses on the test data which will help us to keep\n",
    "# track of how well our model is performing not just on our training data but also on our test data\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = 'adam')\n",
    "\n",
    "model.fit(x = X_train, y = y_train, epochs=200, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.683132</td>\n",
       "      <td>0.671749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646729</td>\n",
       "      <td>0.606025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.595305</td>\n",
       "      <td>0.543974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.536151</td>\n",
       "      <td>0.479521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.475731</td>\n",
       "      <td>0.428346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>1.271264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>1.271397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>1.283842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>1.290807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>1.277155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  val_loss\n",
       "0    0.683132  0.671749\n",
       "1    0.646729  0.606025\n",
       "2    0.595305  0.543974\n",
       "3    0.536151  0.479521\n",
       "4    0.475731  0.428346\n",
       "..        ...       ...\n",
       "195  0.000963  1.271264\n",
       "196  0.000997  1.271397\n",
       "197  0.000912  1.283842\n",
       "198  0.000978  1.290807\n",
       "199  0.000920  1.277155\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/200\n",
      "242/242 [==============================] - 0s 189us/sample - loss: 8.9989e-04 - val_loss: 1.2898\n",
      "Epoch 2/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 8.7532e-04 - val_loss: 1.3034\n",
      "Epoch 3/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 8.2795e-04 - val_loss: 1.2990\n",
      "Epoch 4/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 8.3789e-04 - val_loss: 1.2965\n",
      "Epoch 5/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 8.4209e-04 - val_loss: 1.3089\n",
      "Epoch 6/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 8.1824e-04 - val_loss: 1.3046\n",
      "Epoch 7/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 7.7490e-04 - val_loss: 1.3071\n",
      "Epoch 8/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 8.1400e-04 - val_loss: 1.3140\n",
      "Epoch 9/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 7.9632e-04 - val_loss: 1.3123\n",
      "Epoch 10/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 7.6001e-04 - val_loss: 1.3217\n",
      "Epoch 11/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 8.1168e-04 - val_loss: 1.3276\n",
      "Epoch 12/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 7.1910e-04 - val_loss: 1.3178\n",
      "Epoch 13/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 8.3721e-04 - val_loss: 1.3374\n",
      "Epoch 14/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.0010 - val_loss: 1.3533\n",
      "Epoch 15/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 7.3841e-04 - val_loss: 1.3205\n",
      "Epoch 16/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 9.1780e-04 - val_loss: 1.3415\n",
      "Epoch 17/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 6.8855e-04 - val_loss: 1.3465\n",
      "Epoch 18/200\n",
      "242/242 [==============================] - 0s 185us/sample - loss: 6.5622e-04 - val_loss: 1.3511\n",
      "Epoch 19/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 6.4038e-04 - val_loss: 1.3502\n",
      "Epoch 20/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 6.3762e-04 - val_loss: 1.3571\n",
      "Epoch 21/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 6.4381e-04 - val_loss: 1.3574\n",
      "Epoch 22/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 6.2130e-04 - val_loss: 1.3520\n",
      "Epoch 23/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 6.2335e-04 - val_loss: 1.3650\n",
      "Epoch 24/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 5.9240e-04 - val_loss: 1.3631\n",
      "Epoch 25/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 5.9158e-04 - val_loss: 1.3700\n",
      "Epoch 26/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 5.7544e-04 - val_loss: 1.3741\n",
      "Epoch 27/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 5.7032e-04 - val_loss: 1.3720\n",
      "Epoch 28/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 5.5802e-04 - val_loss: 1.3773\n",
      "Epoch 29/200\n",
      "242/242 [==============================] - 0s 190us/sample - loss: 5.6437e-04 - val_loss: 1.3811\n",
      "Epoch 30/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 5.3582e-04 - val_loss: 1.3795\n",
      "Epoch 31/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 5.8024e-04 - val_loss: 1.3816\n",
      "Epoch 32/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 5.3257e-04 - val_loss: 1.3900\n",
      "Epoch 33/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 5.4188e-04 - val_loss: 1.4001\n",
      "Epoch 34/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 5.1553e-04 - val_loss: 1.3915\n",
      "Epoch 35/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 5.5327e-04 - val_loss: 1.3857\n",
      "Epoch 36/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 5.2381e-04 - val_loss: 1.4021\n",
      "Epoch 37/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 5.0858e-04 - val_loss: 1.4029\n",
      "Epoch 38/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 4.9302e-04 - val_loss: 1.4063\n",
      "Epoch 39/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 4.9003e-04 - val_loss: 1.4039\n",
      "Epoch 40/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 4.7337e-04 - val_loss: 1.4081\n",
      "Epoch 41/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 4.6749e-04 - val_loss: 1.4114\n",
      "Epoch 42/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 4.6253e-04 - val_loss: 1.4106\n",
      "Epoch 43/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 4.6715e-04 - val_loss: 1.4200\n",
      "Epoch 44/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 4.4756e-04 - val_loss: 1.4183\n",
      "Epoch 45/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 4.4082e-04 - val_loss: 1.4195\n",
      "Epoch 46/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 4.4297e-04 - val_loss: 1.4262\n",
      "Epoch 47/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 4.3334e-04 - val_loss: 1.4248\n",
      "Epoch 48/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 4.2871e-04 - val_loss: 1.4332\n",
      "Epoch 49/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 4.2076e-04 - val_loss: 1.4389\n",
      "Epoch 50/200\n",
      "242/242 [==============================] - 0s 173us/sample - loss: 4.1015e-04 - val_loss: 1.4337\n",
      "Epoch 51/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 4.1696e-04 - val_loss: 1.4312\n",
      "Epoch 52/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 4.0407e-04 - val_loss: 1.4407\n",
      "Epoch 53/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 4.2905e-04 - val_loss: 1.4505\n",
      "Epoch 54/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.8738e-04 - val_loss: 1.4451\n",
      "Epoch 55/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.8425e-04 - val_loss: 1.4428\n",
      "Epoch 56/200\n",
      "242/242 [==============================] - 0s 99us/sample - loss: 3.7299e-04 - val_loss: 1.4517\n",
      "Epoch 57/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.7702e-04 - val_loss: 1.4547\n",
      "Epoch 58/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.6540e-04 - val_loss: 1.4593\n",
      "Epoch 59/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.7568e-04 - val_loss: 1.4559\n",
      "Epoch 60/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.6492e-04 - val_loss: 1.4620\n",
      "Epoch 61/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 3.5023e-04 - val_loss: 1.4626\n",
      "Epoch 62/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.4418e-04 - val_loss: 1.4648\n",
      "Epoch 63/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.3729e-04 - val_loss: 1.4696\n",
      "Epoch 64/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.4939e-04 - val_loss: 1.4754\n",
      "Epoch 65/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.3053e-04 - val_loss: 1.4714\n",
      "Epoch 66/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.2663e-04 - val_loss: 1.4738\n",
      "Epoch 67/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.2044e-04 - val_loss: 1.4798\n",
      "Epoch 68/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.2276e-04 - val_loss: 1.4820\n",
      "Epoch 69/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.1484e-04 - val_loss: 1.4812\n",
      "Epoch 70/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.0828e-04 - val_loss: 1.4824\n",
      "Epoch 71/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 3.0542e-04 - val_loss: 1.4872\n",
      "Epoch 72/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 2.9940e-04 - val_loss: 1.4913\n",
      "Epoch 73/200\n",
      "242/242 [==============================] - 0s 103us/sample - loss: 2.9704e-04 - val_loss: 1.4950\n",
      "Epoch 74/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 3.0517e-04 - val_loss: 1.4919\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 87us/sample - loss: 2.8956e-04 - val_loss: 1.4978\n",
      "Epoch 76/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 2.9298e-04 - val_loss: 1.4983\n",
      "Epoch 77/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 2.8624e-04 - val_loss: 1.5006\n",
      "Epoch 78/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 2.8311e-04 - val_loss: 1.5084\n",
      "Epoch 79/200\n",
      "242/242 [==============================] - 0s 91us/sample - loss: 2.8626e-04 - val_loss: 1.5075\n",
      "Epoch 80/200\n",
      "242/242 [==============================] - 0s 111us/sample - loss: 2.7631e-04 - val_loss: 1.5073\n",
      "Epoch 81/200\n",
      "242/242 [==============================] - 0s 95us/sample - loss: 2.7222e-04 - val_loss: 1.5115\n",
      "Epoch 82/200\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 2.8245e-04 - val_loss: 1.5119\n",
      "Epoch 83/200\n",
      "242/242 [==============================] - 0s 103us/sample - loss: 2.6125e-04 - val_loss: 1.5117\n",
      "Epoch 84/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 2.6637e-04 - val_loss: 1.5150\n",
      "Epoch 85/200\n",
      "242/242 [==============================] - 0s 223us/sample - loss: 2.6207e-04 - val_loss: 1.5209\n",
      "Epoch 86/200\n",
      "242/242 [==============================] - 0s 181us/sample - loss: 2.6410e-04 - val_loss: 1.5230\n",
      "Epoch 00086: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c8aba71048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop automatically stop training data based of a loss condition on the validation data passed during the model.fit()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=85) # verbose = 1 : we will see bunch of output\n",
    "\n",
    "# patience : That means we will wait 85 epochs even after we detected a stopping point because of noice that could occur.  \n",
    "# callbacks : This will prevent our model to overfitting\n",
    "\n",
    "model.fit(x = X_train, y = y_train, epochs=200, validation_data=(X_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c8aba957c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/200\n",
      "242/242 [==============================] - 1s 3ms/sample - loss: 0.7531 - val_loss: 0.6911\n",
      "Epoch 2/200\n",
      "242/242 [==============================] - 0s 198us/sample - loss: 0.6831 - val_loss: 0.6922\n",
      "Epoch 3/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.7112 - val_loss: 0.6936\n",
      "Epoch 4/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.6875 - val_loss: 0.6947\n",
      "Epoch 5/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.7118 - val_loss: 0.6946\n",
      "Epoch 6/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6989 - val_loss: 0.6949\n",
      "Epoch 7/200\n",
      "242/242 [==============================] - 0s 194us/sample - loss: 0.6894 - val_loss: 0.6955\n",
      "Epoch 8/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.6889 - val_loss: 0.6952\n",
      "Epoch 9/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6972 - val_loss: 0.6951\n",
      "Epoch 10/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6946 - val_loss: 0.6949\n",
      "Epoch 11/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6860 - val_loss: 0.6953\n",
      "Epoch 12/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6969 - val_loss: 0.6961\n",
      "Epoch 13/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.6981 - val_loss: 0.6966\n",
      "Epoch 14/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6799 - val_loss: 0.6966\n",
      "Epoch 15/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.7064 - val_loss: 0.6964\n",
      "Epoch 16/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.6912 - val_loss: 0.6959\n",
      "Epoch 17/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.6934 - val_loss: 0.6964\n",
      "Epoch 18/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.6816 - val_loss: 0.6966\n",
      "Epoch 19/200\n",
      "242/242 [==============================] - 0s 173us/sample - loss: 0.6820 - val_loss: 0.6973\n",
      "Epoch 20/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.7005 - val_loss: 0.6984\n",
      "Epoch 21/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6879 - val_loss: 0.7005\n",
      "Epoch 22/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6884 - val_loss: 0.7011\n",
      "Epoch 23/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6885 - val_loss: 0.7007\n",
      "Epoch 24/200\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.689 - 0s 148us/sample - loss: 0.6831 - val_loss: 0.7007\n",
      "Epoch 25/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6823 - val_loss: 0.7016\n",
      "Epoch 26/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.6878 - val_loss: 0.7019\n",
      "Epoch 27/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.6960 - val_loss: 0.7022\n",
      "Epoch 28/200\n",
      "242/242 [==============================] - 0s 202us/sample - loss: 0.6844 - val_loss: 0.7028\n",
      "Epoch 29/200\n",
      "242/242 [==============================] - 0s 177us/sample - loss: 0.6766 - val_loss: 0.7038\n",
      "Epoch 30/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6807 - val_loss: 0.7040\n",
      "Epoch 31/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.6888 - val_loss: 0.7041\n",
      "Epoch 32/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6812 - val_loss: 0.7043\n",
      "Epoch 33/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6810 - val_loss: 0.7043\n",
      "Epoch 34/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6956 - val_loss: 0.7012\n",
      "Epoch 35/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.6854 - val_loss: 0.6984\n",
      "Epoch 36/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6907 - val_loss: 0.6967\n",
      "Epoch 37/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.6861 - val_loss: 0.6950\n",
      "Epoch 38/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6896 - val_loss: 0.6967\n",
      "Epoch 39/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6731 - val_loss: 0.6941\n",
      "Epoch 40/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.6831 - val_loss: 0.6897\n",
      "Epoch 41/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.6706 - val_loss: 0.6843\n",
      "Epoch 42/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6967 - val_loss: 0.6813\n",
      "Epoch 43/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.6761 - val_loss: 0.6812\n",
      "Epoch 44/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6819 - val_loss: 0.6758\n",
      "Epoch 45/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6590 - val_loss: 0.6736\n",
      "Epoch 46/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6778 - val_loss: 0.6692\n",
      "Epoch 47/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6655 - val_loss: 0.6671\n",
      "Epoch 48/200\n",
      "242/242 [==============================] - 0s 173us/sample - loss: 0.6745 - val_loss: 0.6610\n",
      "Epoch 49/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.6590 - val_loss: 0.6500\n",
      "Epoch 50/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6627 - val_loss: 0.6396\n",
      "Epoch 51/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.6711 - val_loss: 0.6363\n",
      "Epoch 52/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.6663 - val_loss: 0.6312\n",
      "Epoch 53/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6589 - val_loss: 0.6281\n",
      "Epoch 54/200\n",
      "242/242 [==============================] - 0s 227us/sample - loss: 0.6684 - val_loss: 0.6342\n",
      "Epoch 55/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6643 - val_loss: 0.6353\n",
      "Epoch 56/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6604 - val_loss: 0.6311\n",
      "Epoch 57/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.6259 - val_loss: 0.6159\n",
      "Epoch 58/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6457 - val_loss: 0.6046\n",
      "Epoch 59/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.6496 - val_loss: 0.6010\n",
      "Epoch 60/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.6432 - val_loss: 0.5985\n",
      "Epoch 61/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6357 - val_loss: 0.5958\n",
      "Epoch 62/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.6307 - val_loss: 0.5839\n",
      "Epoch 63/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.6271 - val_loss: 0.5635\n",
      "Epoch 64/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.6134 - val_loss: 0.5406\n",
      "Epoch 65/200\n",
      "242/242 [==============================] - 0s 280us/sample - loss: 0.6220 - val_loss: 0.5406\n",
      "Epoch 66/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.6253 - val_loss: 0.5515\n",
      "Epoch 67/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.5725 - val_loss: 0.5504\n",
      "Epoch 68/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.6226 - val_loss: 0.5461\n",
      "Epoch 69/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.6044 - val_loss: 0.5262\n",
      "Epoch 70/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.5760 - val_loss: 0.5113\n",
      "Epoch 71/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5559 - val_loss: 0.4944\n",
      "Epoch 72/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.5825 - val_loss: 0.4745\n",
      "Epoch 73/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.6191 - val_loss: 0.4684\n",
      "Epoch 74/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5867 - val_loss: 0.4833\n",
      "Epoch 75/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.6229 - val_loss: 0.5071\n",
      "Epoch 76/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.5662 - val_loss: 0.4913\n",
      "Epoch 77/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5669 - val_loss: 0.4669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.5352 - val_loss: 0.4554\n",
      "Epoch 79/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5473 - val_loss: 0.4419\n",
      "Epoch 80/200\n",
      "242/242 [==============================] - 0s 268us/sample - loss: 0.5762 - val_loss: 0.4314\n",
      "Epoch 81/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.5352 - val_loss: 0.4201\n",
      "Epoch 82/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.5828 - val_loss: 0.4035\n",
      "Epoch 83/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5957 - val_loss: 0.4084\n",
      "Epoch 84/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5750 - val_loss: 0.4178\n",
      "Epoch 85/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.5221 - val_loss: 0.3972\n",
      "Epoch 86/200\n",
      "242/242 [==============================] - 0s 115us/sample - loss: 0.6180 - val_loss: 0.4176\n",
      "Epoch 87/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.5274 - val_loss: 0.4233\n",
      "Epoch 88/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.5787 - val_loss: 0.4154\n",
      "Epoch 89/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.5166 - val_loss: 0.3850\n",
      "Epoch 90/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.5442 - val_loss: 0.3687\n",
      "Epoch 91/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.5338 - val_loss: 0.3715\n",
      "Epoch 92/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.5881 - val_loss: 0.3840\n",
      "Epoch 93/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.5660 - val_loss: 0.3986\n",
      "Epoch 94/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.5809 - val_loss: 0.4097\n",
      "Epoch 95/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.5144 - val_loss: 0.4027\n",
      "Epoch 96/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.5551 - val_loss: 0.3917\n",
      "Epoch 97/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5616 - val_loss: 0.3892\n",
      "Epoch 98/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5831 - val_loss: 0.4056\n",
      "Epoch 99/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.5421 - val_loss: 0.4230\n",
      "Epoch 100/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.5577 - val_loss: 0.4050\n",
      "Epoch 101/200\n",
      "242/242 [==============================] - 0s 194us/sample - loss: 0.5012 - val_loss: 0.3938\n",
      "Epoch 102/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.5440 - val_loss: 0.3904\n",
      "Epoch 103/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.4719 - val_loss: 0.3742\n",
      "Epoch 104/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.5724 - val_loss: 0.3758\n",
      "Epoch 105/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.5107 - val_loss: 0.3822\n",
      "Epoch 106/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.4960 - val_loss: 0.3598\n",
      "Epoch 107/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4856 - val_loss: 0.3520\n",
      "Epoch 108/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.4910 - val_loss: 0.3610\n",
      "Epoch 109/200\n",
      "242/242 [==============================] - 0s 165us/sample - loss: 0.5622 - val_loss: 0.3960\n",
      "Epoch 110/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.4906 - val_loss: 0.3838\n",
      "Epoch 111/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.5029 - val_loss: 0.3645\n",
      "Epoch 112/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.5385 - val_loss: 0.3613\n",
      "Epoch 113/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4875 - val_loss: 0.3690\n",
      "Epoch 114/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.4981 - val_loss: 0.3700\n",
      "Epoch 115/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.5124 - val_loss: 0.3683\n",
      "Epoch 116/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4957 - val_loss: 0.3753\n",
      "Epoch 117/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.5220 - val_loss: 0.3736\n",
      "Epoch 118/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.5424 - val_loss: 0.3722\n",
      "Epoch 119/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4890 - val_loss: 0.3540\n",
      "Epoch 120/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.5277 - val_loss: 0.3517\n",
      "Epoch 121/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.4749 - val_loss: 0.3494\n",
      "Epoch 122/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.5220 - val_loss: 0.3490\n",
      "Epoch 123/200\n",
      "242/242 [==============================] - 0s 169us/sample - loss: 0.4941 - val_loss: 0.3474\n",
      "Epoch 124/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4912 - val_loss: 0.3356\n",
      "Epoch 125/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.5295 - val_loss: 0.3368\n",
      "Epoch 126/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4612 - val_loss: 0.3441\n",
      "Epoch 127/200\n",
      "242/242 [==============================] - 0s 148us/sample - loss: 0.4775 - val_loss: 0.3468\n",
      "Epoch 128/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.4779 - val_loss: 0.3323\n",
      "Epoch 129/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4981 - val_loss: 0.3252\n",
      "Epoch 130/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.4288 - val_loss: 0.3183\n",
      "Epoch 131/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.4925 - val_loss: 0.3141\n",
      "Epoch 132/200\n",
      "242/242 [==============================] - 0s 153us/sample - loss: 0.5188 - val_loss: 0.3246\n",
      "Epoch 133/200\n",
      "242/242 [==============================] - 0s 177us/sample - loss: 0.4738 - val_loss: 0.3185\n",
      "Epoch 134/200\n",
      "242/242 [==============================] - 0s 194us/sample - loss: 0.4500 - val_loss: 0.3153\n",
      "Epoch 135/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.4565 - val_loss: 0.3172\n",
      "Epoch 136/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.5099 - val_loss: 0.3284\n",
      "Epoch 137/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.4897 - val_loss: 0.3420\n",
      "Epoch 138/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.4761 - val_loss: 0.3317\n",
      "Epoch 139/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.4572 - val_loss: 0.3215\n",
      "Epoch 140/200\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.5088 - val_loss: 0.3149\n",
      "Epoch 141/200\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.4952 - val_loss: 0.3190\n",
      "Epoch 142/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4607 - val_loss: 0.3191\n",
      "Epoch 143/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4579 - val_loss: 0.3177\n",
      "Epoch 144/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4544 - val_loss: 0.3100\n",
      "Epoch 145/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4496 - val_loss: 0.3070\n",
      "Epoch 146/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4549 - val_loss: 0.3116\n",
      "Epoch 147/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.5758 - val_loss: 0.3288\n",
      "Epoch 148/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4745 - val_loss: 0.3417\n",
      "Epoch 149/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.4535 - val_loss: 0.3338\n",
      "Epoch 150/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4474 - val_loss: 0.3260\n",
      "Epoch 151/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4414 - val_loss: 0.3236\n",
      "Epoch 152/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4777 - val_loss: 0.3244\n",
      "Epoch 153/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.5062 - val_loss: 0.3283\n",
      "Epoch 154/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.4431 - val_loss: 0.3350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4706 - val_loss: 0.3374\n",
      "Epoch 156/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4171 - val_loss: 0.3356\n",
      "Epoch 157/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4502 - val_loss: 0.3283\n",
      "Epoch 158/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4463 - val_loss: 0.3285\n",
      "Epoch 159/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4569 - val_loss: 0.3262\n",
      "Epoch 160/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4439 - val_loss: 0.3171\n",
      "Epoch 161/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4654 - val_loss: 0.3095\n",
      "Epoch 162/200\n",
      "242/242 [==============================] - 0s 202us/sample - loss: 0.4616 - val_loss: 0.3141\n",
      "Epoch 163/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4680 - val_loss: 0.3179\n",
      "Epoch 164/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4780 - val_loss: 0.3128\n",
      "Epoch 165/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4566 - val_loss: 0.3113\n",
      "Epoch 166/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4182 - val_loss: 0.3082\n",
      "Epoch 167/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4457 - val_loss: 0.3076\n",
      "Epoch 168/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.4437 - val_loss: 0.3111\n",
      "Epoch 169/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4146 - val_loss: 0.3159\n",
      "Epoch 170/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4435 - val_loss: 0.3149\n",
      "Epoch 171/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4854 - val_loss: 0.3105\n",
      "Epoch 172/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4164 - val_loss: 0.3123\n",
      "Epoch 173/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4078 - val_loss: 0.3138\n",
      "Epoch 174/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4357 - val_loss: 0.3085\n",
      "Epoch 175/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.4732 - val_loss: 0.3049\n",
      "Epoch 176/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4369 - val_loss: 0.3153\n",
      "Epoch 177/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4309 - val_loss: 0.3146\n",
      "Epoch 178/200\n",
      "242/242 [==============================] - 0s 152us/sample - loss: 0.4563 - val_loss: 0.3193\n",
      "Epoch 179/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4013 - val_loss: 0.3189\n",
      "Epoch 180/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.4506 - val_loss: 0.3146\n",
      "Epoch 181/200\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.4434 - val_loss: 0.3140\n",
      "Epoch 182/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.4433 - val_loss: 0.3069\n",
      "Epoch 183/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.3954 - val_loss: 0.3008\n",
      "Epoch 184/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.4338 - val_loss: 0.3080\n",
      "Epoch 185/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.3853 - val_loss: 0.3142\n",
      "Epoch 186/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4255 - val_loss: 0.3295\n",
      "Epoch 187/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.4377 - val_loss: 0.3435\n",
      "Epoch 188/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.5022 - val_loss: 0.3283\n",
      "Epoch 189/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4033 - val_loss: 0.3293\n",
      "Epoch 190/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.4006 - val_loss: 0.3306\n",
      "Epoch 191/200\n",
      "242/242 [==============================] - 0s 181us/sample - loss: 0.3915 - val_loss: 0.3333\n",
      "Epoch 192/200\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.4167 - val_loss: 0.3266\n",
      "Epoch 193/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.4338 - val_loss: 0.3205\n",
      "Epoch 194/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.3655 - val_loss: 0.3283\n",
      "Epoch 195/200\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.4189 - val_loss: 0.3317\n",
      "Epoch 196/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.4585 - val_loss: 0.3245\n",
      "Epoch 197/200\n",
      "242/242 [==============================] - 0s 124us/sample - loss: 0.4016 - val_loss: 0.3258\n",
      "Epoch 198/200\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.3954 - val_loss: 0.3194\n",
      "Epoch 199/200\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.3850 - val_loss: 0.3196\n",
      "Epoch 200/200\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.3904 - val_loss: 0.3208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c8ac131c88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout help us to prevent overfitting\n",
    "\n",
    "\n",
    "\n",
    "# Droupout : The term â€œdropoutâ€ refers to dropping out units (both hidden and visible) in a neural network.\n",
    "# (0.5) is rate : Rate is the probability that we are going to randomly turnoff the actual neuron (0 : None, 1 : 100%)\n",
    "\n",
    "# BackPropogation : Updating the weights and bias  \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(100,activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(50,activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(25,activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(15,activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(5,activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "model1.add(Dense(1,activation='sigmoid'))    # sigmoid : Becasue it is a BINARY-CLASSIFICATION problem\n",
    "\n",
    "\n",
    "# After each epochs of training our X_train we will run test data check our losses on the test data which will help us to keep\n",
    "# track of how well our model is performing not just on our training data but also on our test data\n",
    "\n",
    "model1.compile(loss = \"binary_crossentropy\",optimizer='adam')\n",
    "\n",
    "model1.fit(x = X_train, y = y_train, epochs=200, validation_data=(X_test,y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c8abdc9c48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploring what my model history looks like\n",
    "\n",
    "# model.history.history : Gives History of our losses \n",
    "\n",
    "\n",
    "# loss = Loss of my training set\n",
    "# val_loss = Loss of my test set\n",
    "\n",
    "model1_loss = pd.DataFrame(model1.history.history)\n",
    "model1_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32080178084920663"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will return our models loss\n",
    "\n",
    "model1.evaluate(X_test,y_test,verbose=0)    # verbose = 0 : we will not see bunch of output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.92        32\n",
      "           1       0.85      1.00      0.92        29\n",
      "\n",
      "    accuracy                           0.92        61\n",
      "   macro avg       0.93      0.92      0.92        61\n",
      "weighted avg       0.93      0.92      0.92        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To check the accuracy of the model : Column : f1-score , Row : accuracy --> 90%\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  5]\n",
      " [ 0 29]]\n"
     ]
    }
   ],
   "source": [
    "# For detail explanation of confusion_matrix } https://www.infinitycodex.in/confusion-matrix-and-classification\n",
    "\n",
    "# Our model only miss classify only 2 points in the test set.\n",
    "\n",
    "print(confusion_matrix(y_test,pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the accuracy of BPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9180327868852459"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
